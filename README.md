<img width="1485" height="815" alt="image" src="https://github.com/user-attachments/assets/600a376c-252a-48c8-bb15-02717918ae40" />
# ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (Final Project Report)
## ‡∏ß‡∏¥‡∏ä‡∏≤ 204466 ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å (Deep Learning)

---

## ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ
**Segmented Urban Street to Realistic Urban Street with Pix2Pix GauGANs**

*A take on Nvidia's GauGANs with pix2pix architecture and urban street dataset*

---


**‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏á‡∏≤‡∏ô:**
- ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 1 (50%): Dataset Preparation & Organization, Model Architecture Development
- ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 2 (50%): Model Training & Optimization, Evaluation, Documentation

---

## 1. ‡∏ö‡∏ó‡∏ô‡∏≥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ
<div style="display: flex; gap: 20px; justify-content: center; flex-wrap: nowrap;">

  <div style="text-align: center;">
    <p><strong>Input</strong></p>
    <img src="https://github.com/user-attachments/assets/ca624ada-96cf-4728-9def-22b6c6b0551e" 
         alt="Input Image" style="max-width: 400px; border: 1px solid #ccc; border-radius: 8px;">
  </div>

  <div style="text-align: center;">
    <p><strong>Output</strong></p>
    <img src="https://cdn.discordapp.com/attachments/1113666073221406770/1435297322018799870/image.png?ex=690b744c&is=690a22cc&hm=70ea674b973aeba0f228ffab2a6abf28d4e016c6f2c963f2fddfa690badb7278" 
         alt="Output Image" style="max-width: 400px; border: 1px solid #ccc; border-radius: 8px;">
  </div>

  <div style="text-align: center;">
    <p><strong>Ground Truth</strong></p>
    <img src="https://media.discordapp.net/attachments/1113666073221406770/1435297728493125775/image.png?ex=690b74ad&is=690a232d&hm=ebe35f4362e30c7b486d021d19f056bb7e05cce241c395c1544164f767828cff&format=webp&quality=lossless&width=1538&height=859" 
         alt="Ground Truth Image" style="max-width: 400px; border: 1px solid #ccc; border-radius: 8px;">
  </div>

</div>



### 1.1 ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ Seg2Scene ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ Deep Learning ‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏û Semantic Segmentation Map ‡∏Ç‡∏≠‡∏á‡∏ñ‡∏ô‡∏ô‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á (Urban Street) ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á (Photorealistic) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ **Pix2Pix Generative Adversarial Networks (GANs)**

‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å Semantic Labels ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏™‡∏≤‡∏Ç‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á (Urban Planning), ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Autonomous Driving), ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á (Virtual Environment) ‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å Segmentation Map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å

### 1.2 ‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ

‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

1. **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ**: ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å Semantic Segmentation Map ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Image-to-Image Translation ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ GANs

2. **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á**:
   - **Urban Planning**: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡∏ú‡∏π‡πâ‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡πÑ‡∏î‡πâ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏™‡∏µ‡∏¢
   - **Autonomous Driving**: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
   - **Game Development & Virtual Environments**: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏°‡πÅ‡∏•‡∏∞‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô VR/AR

3. **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á GANs**: GANs ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô Deep Learning ‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏î‡πâ

4. **‡∏°‡∏µ Dataset ‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞**: ‡∏°‡∏µ **Cityscape Dataset** ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô Semantic Segmentation ‡πÅ‡∏•‡∏∞ Image Synthesis ‡∏Ç‡∏≠‡∏á‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡πÄ‡∏°‡∏∑‡∏≠‡∏á

---

## 2. ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Deep Learning

### 2.1 ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô

| ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ | ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ |
|--------|---------|-------|---------|
| **Traditional CV** | Template Matching, Morphing | - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á training<br>- ‡πÄ‡∏£‡πá‡∏ß | - ‡πÑ‡∏°‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô<br>- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏π‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥<br>- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ template ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á |
| **Rule-based** | Hand-crafted features + Rules | - ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÑ‡∏î‡πâ<br>- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ | - ‡πÑ‡∏°‡πà generalize<br>- ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö rules ‡πÄ‡∏¢‡∏≠‡∏∞<br>- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≥‡∏Å‡∏±‡∏î |
| **Deep Learning (Pix2Pix GAN)** | Neural Network Learning | - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô<br>- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á<br>- Generalize ‡πÑ‡∏î‡πâ‡∏î‡∏µ<br>- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î rules | - ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞<br>- Training ‡∏ô‡∏≤‡∏ô<br>- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ GPU<br>- Black box |

### 2.2 ‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á Deep Learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ

1. **‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô**: Deep Learning ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Semantic Labels ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™ ‡πÅ‡∏™‡∏á‡πÄ‡∏á‡∏≤ ‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£ ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î rules ‡πÄ‡∏≠‡∏á

2. **‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á**: GANs ‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡πÅ‡∏•‡∏∞‡∏î‡∏π‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Adversarial Training ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á

3. **Generalization**: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö Segmentation Map ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ template ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£

4. **End-to-End Learning**: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• input-output pairs ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡πÜ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î texture rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ class

### 2.3 ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á Deep Learning

1. **‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å**: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ dataset ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (~3,000 training images ‡∏à‡∏≤‡∏Å Cityscape Dataset) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ

2. **‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Training**: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô (60 epochs) ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ GPU ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Ç‡∏ô‡∏≤‡∏î 1024√ó2048

3. **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**: ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏≤‡∏à‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ artifacts ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ö‡∏¥‡∏î‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏ó‡∏£‡∏á‡∏ú‡∏¥‡∏î‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô

4. **Black Box Nature**: ‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏∂‡∏á‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡πâ‡∏ô

---

## 3. ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° Deep Learning ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ **Pix2Pix GAN** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Conditional GAN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô Image-to-Image Translation ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ 2 ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å:

### 3.1 ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Pix2Pix GAN

```
Input (Segmentation Map) ‚Üí Generator (U-Net) ‚Üí Generated Urban Street ‚Üí Discriminator ‚Üí Real/Fake
                                                        ‚Üë
Ground Truth Urban Street ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

```mermaid
graph LR
    A[Input: Segmentation Map<br/>1024√ó2048√ó3] --> B[Generator<br/>U-Net]
    B --> C[Generated Urban Street<br/>1024√ó2048√ó3]
    D[Ground Truth Urban Street<br/>1024√ó2048√ó3] --> E[Discriminator<br/>PatchGAN]
    C --> E
    A --> E
    E --> F{Real or Fake?<br/>30√ó30 patches}
    F -->|Real| G[Label: 1]
    F -->|Fake| H[Label: 0]

    style A fill:#e1f5ff
    style B fill:#ffe1f5
    style C fill:#e1ffe1
    style D fill:#e1ffe1
    style E fill:#fff5e1
    style F fill:#f5e1ff
```

### 3.2 Generator: U-Net Architecture

Generator ‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° **U-Net** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Encoder-Decoder network ‡∏û‡∏£‡πâ‡∏≠‡∏° Skip Connections

**‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á:**

```
Input (Segmentation Map: 1024√ó2048√ó3)
    ‚Üì
[Encoder - Contracting Path]
    7 Layers ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡∏∞‡∏Ñ‡∏£‡∏∂‡πà‡∏á, ‡πÄ‡∏û‡∏¥‡πà‡∏° features 64‚Üí512
    ‚Üì
[Bottleneck]
    Feature extraction ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (512 channels)
    ‚Üì
[Decoder - Expansive Path]
    7 Layers ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏±‡∏ö, ‡∏•‡∏î features 512‚Üí64
    ‡∏°‡∏µ Skip Connections ‡∏à‡∏≤‡∏Å Encoder
    ‚Üì
Output Conv + Tanh ‚Üí Realistic Urban Street (1024√ó2048√ó3)
```

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏**: ‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏Ñ‡∏∑‡∏≠ 1024√ó2048 ‡πÅ‡∏ï‡πà‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° U-Net ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô Fully Convolutional Network

**‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:**
- **Encoder (7 layers)**: ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏•‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡∏∞‡∏Ñ‡∏£‡∏∂‡πà‡∏á ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features (64 ‚Üí 512)
- **Skip Connections**: ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ encoder layers ‡∏Å‡∏±‡∏ö decoder layers ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
- **Decoder (7 layers)**: ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features (512 ‚Üí 64)
- **Dropout**: ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô 3 decoder layers ‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting
- **Activation Functions**:
  - Encoder: LeakyReLU (Œ±=0.2)
  - Decoder: ReLU
  - Output: Tanh (‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤ -1 ‡∏ñ‡∏∂‡∏á 1)

#### Diagram: U-Net Architecture

```mermaid
graph TD
    Input[Input: Segmentation Map<br/>1024√ó2048√ó3] --> E1[Conv 64<br/>LeakyReLU<br/>512√ó1024√ó64]
    E1 --> E2[Conv 128<br/>BatchNorm + LeakyReLU<br/>64√ó64√ó128]
    E2 --> E3[Conv 256<br/>BatchNorm + LeakyReLU<br/>32√ó32√ó256]
    E3 --> E4[Conv 512<br/>BatchNorm + LeakyReLU<br/>16√ó16√ó512]
    E4 --> E5[Conv 512<br/>BatchNorm + LeakyReLU<br/>8√ó8√ó512]
    E5 --> E6[Conv 512<br/>BatchNorm + LeakyReLU<br/>4√ó4√ó512]
    E6 --> E7[Conv 512<br/>BatchNorm + LeakyReLU<br/>2√ó2√ó512]

    E7 --> B[Bottleneck<br/>Conv 512 + ReLU<br/>1√ó1√ó512]

    B --> D1[DeConv 512<br/>BatchNorm + Dropout + ReLU<br/>2√ó2√ó512]
    E7 -.Skip Connection.-> D1
    D1 --> D2[DeConv 512<br/>BatchNorm + Dropout + ReLU<br/>4√ó4√ó512]
    E6 -.Skip Connection.-> D2
    D2 --> D3[DeConv 512<br/>BatchNorm + Dropout + ReLU<br/>8√ó8√ó512]
    E5 -.Skip Connection.-> D3
    D3 --> D4[DeConv 512<br/>BatchNorm + ReLU<br/>16√ó16√ó512]
    E4 -.Skip Connection.-> D4
    D4 --> D5[DeConv 256<br/>BatchNorm + ReLU<br/>32√ó32√ó256]
    E3 -.Skip Connection.-> D5
    D5 --> D6[DeConv 128<br/>BatchNorm + ReLU<br/>64√ó64√ó128]
    E2 -.Skip Connection.-> D6
    D6 --> D7[DeConv 64<br/>BatchNorm + ReLU<br/>128√ó128√ó64]
    E1 -.Skip Connection.-> D7

    D7 --> Output[Output: Realistic Urban Street<br/>1024√ó2048√ó3]

    style Input fill:#e1f5ff
    style B fill:#ffe1e1
    style Output fill:#e1ffe1
    style E1 fill:#ffeaa7
    style E2 fill:#ffeaa7
    style E3 fill:#ffeaa7
    style E4 fill:#ffeaa7
    style E5 fill:#ffeaa7
    style E6 fill:#ffeaa7
    style E7 fill:#ffeaa7
    style D1 fill:#dfe6e9
    style D2 fill:#dfe6e9
    style D3 fill:#dfe6e9
    style D4 fill:#dfe6e9
    style D5 fill:#dfe6e9
    style D6 fill:#dfe6e9
    style D7 fill:#dfe6e9
```

### 3.3 Discriminator: PatchGAN Architecture

Discriminator ‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° **PatchGAN** ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô patches ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏û

**‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á:**

```
Input: Concatenate(Segmentation Map, Urban Street) ‚Üí 1024√ó2048√ó6
    ‚Üì
Conv 64, stride 2 + LeakyReLU ‚Üí 128√ó128√ó64
    ‚Üì
Conv 128, stride 2 + BatchNorm + LeakyReLU ‚Üí 64√ó64√ó128
    ‚Üì
Conv 256, stride 2 + BatchNorm + LeakyReLU ‚Üí 32√ó32√ó256
    ‚Üì
Conv 512, stride 1 + BatchNorm + LeakyReLU ‚Üí 32√ó32√ó512
    ‚Üì
Conv 1, stride 1 ‚Üí 30√ó30√ó1 (Patch predictions)
```

**‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:**
- **Input**: Segmentation Map ‡πÅ‡∏•‡∏∞ Urban Street image ‡∏ñ‡∏π‡∏Å concatenate ‡πÄ‡∏õ‡πá‡∏ô 6 channels
- **4 CNN Blocks**: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á features [64, 128, 256, 512]
- **Output**: Feature map (patch-based) ‡πÇ‡∏î‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞ pixel ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤ patch ‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Real ‡∏´‡∏£‡∏∑‡∏≠ Fake
- **Patch Size**: 70√ó70 pixels (receptive field ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ output pixel)

#### Diagram: PatchGAN Architecture

```mermaid
graph TD
    Seg[Segmentation Map<br/>1024√ó2048√ó3] --> Concat[Concatenate<br/>1024√ó2048√ó6]
    Land[Urban Street Image<br/>1024√ó2048√ó3] --> Concat

    Concat --> C1[Conv 64, stride=2<br/>LeakyReLU<br/>512√ó1024√ó64]
    C1 --> C2[Conv 128, stride=2<br/>BatchNorm + LeakyReLU<br/>256√ó512√ó128]
    C2 --> C3[Conv 256, stride=2<br/>BatchNorm + LeakyReLU<br/>128√ó256√ó256]
    C3 --> C4[Conv 512, stride=1<br/>BatchNorm + LeakyReLU<br/>128√ó256√ó512]
    C4 --> Out[Conv 1, stride=1<br/>Patch Predictions]

    Out --> Patch[Patch-based Output<br/>Each pixel = Real/Fake<br/>for 70√ó70 patch]

    style Seg fill:#e1f5ff
    style Land fill:#e1ffe1
    style Concat fill:#ffe1f5
    style C1 fill:#ffeaa7
    style C2 fill:#ffeaa7
    style C3 fill:#ffeaa7
    style C4 fill:#ffeaa7
    style Out fill:#dfe6e9
    style Patch fill:#f5e1ff
```

### 3.4 Loss Functions

**Generator Loss:**
```
L_total = L_GAN + Œª √ó L_L1

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:
L_GAN = BCE(D(G(x)), 1)  # Adversarial loss
L_L1 = ||y - G(x)||‚ÇÅ      # L1 pixel-wise loss
Œª = 100                    # L1 loss weight
```

**Discriminator Loss:**
```
L_D = (BCE(D(x,y), 1) + BCE(D(x,G(x)), 0)) / 2

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:
D(x,y) = Discriminator(segmentation_map, real_landscape)
D(x,G(x)) = Discriminator(segmentation_map, generated_landscape)
```

**BCE (Binary Cross Entropy):**
```
BCE(≈∑, y) = -[y log(≈∑) + (1-y) log(1-≈∑)]
```

### 3.5 Training Process Diagram

```mermaid
graph TB
    A[Input: Segmentation Map] --> B[Generator U-Net]
    B --> C[Generated Urban Street]
    D[Real Urban Street] --> E[Discriminator]
    C --> E
    A --> E
    E --> F[Real/Fake Prediction]
    F --> G[BCE Loss for D]
    C --> H[L1 Loss]
    D --> H
    F --> I[GAN Loss for G]
    G --> J[Update Discriminator]
    H --> K[Combined Loss for G]
    I --> K
    K --> L[Update Generator]
```

#### Diagram: Detailed Training Process

```mermaid
graph TB
    subgraph Input ["üì• Input Data"]
        SegBatch[Segmentation Map Batch<br/>x: 8√ó3√ó1024√ó2048]
        RealBatch[Real Urban Street Batch<br/>y: 8√ó3√ó1024√ó2048]
    end

    subgraph GenPhase ["üé® Generator Forward Pass"]
        SegBatch --> Gen[Generator U-Net<br/>7 Encoder + Bottleneck<br/>+ 7 Decoder]
        Gen --> FakeLand[Generated Urban Street<br/>y_fake: 8√ó3√ó1024√ó2048]
    end

    subgraph DiscPhase1 ["üîç Discriminator - Phase 1: Train D"]
        SegBatch --> ConcatReal[Concatenate<br/>Seg + Real]
        RealBatch --> ConcatReal
        ConcatReal --> DiscReal[Discriminator<br/>PatchGAN]
        DiscReal --> PredReal[Prediction: Real<br/>D_real: patch-based]

        SegBatch --> ConcatFake[Concatenate<br/>Seg + Fake]
        FakeFake[Generated Urban Street<br/>.detach] --> ConcatFake
        ConcatFake --> DiscFake[Discriminator<br/>PatchGAN]
        DiscFake --> PredFake[Prediction: Fake<br/>D_fake: patch-based]

        PredReal --> LossRealD[BCE Loss<br/>target = 1]
        PredFake --> LossFakeD[BCE Loss<br/>target = 0]
        LossRealD --> AvgD[Average<br/>D_loss]
        LossFakeD --> AvgD
        AvgD --> UpdateD[üîÑ Update D<br/>Backprop + Optimizer]
    end

    FakeLand --> FakeFake

    subgraph DiscPhase2 ["üéØ Discriminator - Phase 2: Evaluate for G"]
        SegBatch --> ConcatFake2[Concatenate<br/>Seg + Fake]
        FakeLand --> ConcatFake2
        ConcatFake2 --> DiscFake2[Discriminator<br/>PatchGAN]
        DiscFake2 --> PredFake2[Prediction<br/>D_fake: patch-based]
    end

    subgraph GenLoss ["üìä Generator Loss Computation"]
        PredFake2 --> GANLoss[GAN Loss<br/>BCE target = 1<br/>Fool Discriminator]
        FakeLand --> L1Comp[L1 Distance]
        RealBatch --> L1Comp
        L1Comp --> L1Loss[L1 Loss √ó 100<br/>Reconstruction]
        GANLoss --> CombineG[Combined Loss<br/>G_total]
        L1Loss --> CombineG
        CombineG --> UpdateG[üîÑ Update G<br/>Backprop + Optimizer]
    end

    subgraph Results ["üìà Training Metrics"]
        UpdateD --> Metrics[Monitor Losses<br/>D_loss: 0.3-0.4<br/>G_loss: 10-15]
        UpdateG --> Metrics
        Metrics --> NextBatch[Next Batch]
    end

    style SegBatch fill:#e1f5ff
    style RealBatch fill:#e1ffe1
    style Gen fill:#ffe1f5
    style FakeLand fill:#ffeaa7
    style DiscReal fill:#fff5e1
    style DiscFake fill:#fff5e1
    style DiscFake2 fill:#fff5e1
    style UpdateD fill:#74b9ff
    style UpdateG fill:#a29bfe
    style Metrics fill:#dfe6e9
```

---

## 4. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î PyTorch

‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Jupyter Notebook: **`Untitled copy.ipynb`**

GitHub Repository: **https://github.com/PluzNtp/Edge-to-Face**

### 4.1 ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å:
- **`Untitled copy.ipynb`**: Jupyter Notebook ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Dataset Loading, Model Training, ‡πÅ‡∏•‡∏∞ Inference

### 4.2 ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: Dataset Loading

#### 4.2.1 Dataset Class (UrbanStreetDataset)

```python
import os
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np

class UrbanStreetDataset(Dataset):
    """
    Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Cityscape: Segmentation Map ‚Üí Realistic Urban Street
    """
    def __init__(self, root_dir):
        self.input_dir = os.path.join(root_dir, "input")   # Segmentation maps
        self.label_dir = os.path.join(root_dir, "label")   # Real photos

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å *_gtFine_color.png
        self.image_names = [
            f.replace("_gtFine_color.png", "")
            for f in os.listdir(self.input_dir)
            if f.endswith("_gtFine_color.png")
        ]

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, index):
        name = self.image_names[index]

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö input ‡πÅ‡∏•‡∏∞ label
        input_path = os.path.join(self.input_dir, f"{name}_gtFine_color.png")
        label_path = os.path.join(self.label_dir, f"{name}_leftImg8bit.png")

        # ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô RGB
        input_image = np.array(Image.open(input_path).convert("RGB"))
        target_image = np.array(Image.open(label_path).convert("RGB"))

        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Tensor: (H, W, C) ‚Üí (C, H, W)
        input_tensor = torch.from_numpy(input_image).permute(2, 0, 1).float() / 255.0
        target_tensor = torch.from_numpy(target_image).permute(2, 0, 1).float() / 255.0

        return input_tensor, target_tensor
```

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- ‡πÇ‡∏´‡∏•‡∏î Segmentation Map ‡∏à‡∏≤‡∏Å `input/` ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å `label/`
- ‡πÑ‡∏ü‡∏•‡πå input: `*_gtFine_color.png` (semantic labels)
- ‡πÑ‡∏ü‡∏•‡πå target: `*_leftImg8bit.png` (real photos)
- ‡πÅ‡∏õ‡∏•‡∏á NumPy array ‚Üí PyTorch Tensor
- Normalize: ‡πÅ‡∏ö‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢ 255.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏ä‡πà‡∏ß‡∏á [0, 1]
- ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ data augmentation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå

#### 4.2.2 ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Dataset

```python
from torch.utils.data import DataLoader

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset
train_dataset = UrbanStreetDataset(root_dir="/workspace/train/data")
val_dataset = UrbanStreetDataset(root_dir="/workspace/val/data")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=8,
    shuffle=True,
    num_workers=2
)

val_loader = DataLoader(
    val_dataset,
    batch_size=8,
    shuffle=False,
    num_workers=2
)
```

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- ‡πÉ‡∏ä‡πâ `DataLoader` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö batch loading ‡πÅ‡∏•‡∏∞ shuffle
- `batch_size=8`: ‡πÇ‡∏´‡∏•‡∏î 8 ‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≠ batch
- `shuffle=True`: ‡∏™‡∏∏‡πà‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô training set
- `num_workers=2`: ‡πÉ‡∏ä‡πâ 2 processes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö parallel data loading

### 4.3 ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: Model Architecture

#### 4.3.1 Building Blocks

```python
# Cell 37 - Basic Block for Generator
class Block(nn.Module):
    def __init__(self, in_channels, out_channels, down=True, act="relu", use_dropout=False):
        super(Block, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode="reflect")
            if down
            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU() if act == "relu" else nn.LeakyReLU(0.2),
        )
        self.use_dropout = use_dropout
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.conv(x)
        return self.dropout(x) if self.use_dropout else x
```

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- **Block** ‡∏Ñ‡∏∑‡∏≠ building block ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
- `down=True`: Encoder block (Conv2d)
- `down=False`: Decoder block (ConvTranspose2d)
- ‡∏°‡∏µ option ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dropout ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å activation function

#### 4.3.2 Generator (U-Net)

```python
# Cell 38 - Generator
class Generator(nn.Module):
    def __init__(self, in_channels=3, features=64):
        super().__init__()

        # Encoder
        self.initial_down = nn.Sequential(
            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode="reflect"),
            nn.LeakyReLU(0.2),
        )
        self.down1 = Block(features, features * 2, down=True, act="leaky", use_dropout=False)
        self.down2 = Block(features * 2, features * 4, down=True, act="leaky", use_dropout=False)
        self.down3 = Block(features * 4, features * 8, down=True, act="leaky", use_dropout=False)
        self.down4 = Block(features * 8, features * 8, down=True, act="leaky", use_dropout=False)
        self.down5 = Block(features * 8, features * 8, down=True, act="leaky", use_dropout=False)
        self.down6 = Block(features * 8, features * 8, down=True, act="leaky", use_dropout=False)

        # Bottleneck
        self.bottleneck = nn.Sequential(
            nn.Conv2d(features * 8, features * 8, 4, 2, 1),
            nn.ReLU()
        )

        # Decoder with skip connections
        self.up1 = Block(features * 8, features * 8, down=False, act="relu", use_dropout=True)
        self.up2 = Block(features * 8 * 2, features * 8, down=False, act="relu", use_dropout=True)
        self.up3 = Block(features * 8 * 2, features * 8, down=False, act="relu", use_dropout=True)
        self.up4 = Block(features * 8 * 2, features * 8, down=False, act="relu", use_dropout=False)
        self.up5 = Block(features * 8 * 2, features * 4, down=False, act="relu", use_dropout=False)
        self.up6 = Block(features * 4 * 2, features * 2, down=False, act="relu", use_dropout=False)
        self.up7 = Block(features * 2 * 2, features, down=False, act="relu", use_dropout=False)

        self.final_up = nn.Sequential(
            nn.ConvTranspose2d(features * 2, in_channels, kernel_size=4, stride=2, padding=1),
            nn.Tanh(),
        )

    def forward(self, x):
        # Encoder
        d1 = self.initial_down(x)
        d2 = self.down1(d1)
        d3 = self.down2(d2)
        d4 = self.down3(d3)
        d5 = self.down4(d4)
        d6 = self.down5(d5)
        d7 = self.down6(d6)

        # Bottleneck
        bottleneck = self.bottleneck(d7)

        # Decoder with skip connections
        up1 = self.up1(bottleneck)
        up2 = self.up2(torch.cat([up1, d7], 1))
        up3 = self.up3(torch.cat([up2, d6], 1))
        up4 = self.up4(torch.cat([up3, d5], 1))
        up5 = self.up5(torch.cat([up4, d4], 1))
        up6 = self.up6(torch.cat([up5, d3], 1))
        up7 = self.up7(torch.cat([up6, d2], 1))

        return self.final_up(torch.cat([up7, d1], 1))
```

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- **Encoder**: 7 layers ‡∏•‡∏î spatial dimensions ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° features
- **Bottleneck**: Layer ‡∏ï‡∏£‡∏á‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ spatial resolution ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î
- **Decoder**: 7 layers ‡∏Ç‡∏¢‡∏≤‡∏¢ spatial dimensions ‡∏Å‡∏•‡∏±‡∏ö
- **Skip Connections**: `torch.cat()` ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° encoder features ‡∏Å‡∏±‡∏ö decoder
- **Output**: Tanh activation ‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ [-1, 1]

#### 4.3.3 Discriminator (PatchGAN)

```python
# Cell 39 - CNN Block for Discriminator
class CNNBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super(CNNBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=False, padding_mode="reflect"),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(0.2),
        )

    def forward(self, x):
        return self.conv(x)

# Cell 40 - Discriminator
class Discriminator(nn.Module):
    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):
        super().__init__()

        self.initial = nn.Sequential(
            nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1, padding_mode="reflect"),
            nn.LeakyReLU(0.2),
        )

        layers = []
        in_channels = features[0]
        for feature in features[1:]:
            layers.append(CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2))
            in_channels = feature

        layers.append(
            nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode="reflect")
        )

        self.model = nn.Sequential(*layers)

    def forward(self, x, y):
        x = torch.cat([x, y], dim=1)  # Concatenate edge + face
        x = self.initial(x)
        return self.model(x)
```

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- **Input**: Concatenate edge (3 channels) ‡πÅ‡∏•‡∏∞ face (3 channels) = 6 channels
- **CNN Blocks**: 4 blocks ‡πÄ‡∏û‡∏¥‡πà‡∏° features [64, 128, 256, 512]
- **Output**: Feature map 30√ó30√ó1 (patch predictions)
- **No Sigmoid**: Output ‡πÄ‡∏õ‡πá‡∏ô logits (‡πÉ‡∏ä‡πâ BCEWithLogitsLoss)

### 4.4 ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Training Loop

#### 4.4.1 Configuration

```python
# Cell 32 - Config
class config:
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    TRAIN_DIR = "/content/drive/MyDrive/Dataset/train"
    VAL_DIR = "/content/drive/MyDrive/Dataset/val"
    LEARNING_RATE = 2e-4
    BATCH_SIZE = 4
    NUM_WORKERS = 2
    IMAGE_SIZE = 256
    CHANNELS_IMG = 3
    L1_LAMBDA = 100
    NUM_EPOCHS = 1
    LOAD_MODEL = False
    SAVE_MODEL = True
    CHECKPOINT_DISC = "/content/drive/MyDrive/checkpoint/disc.pth.tar"
    CHECKPOINT_GEN = "/content/drive/MyDrive/checkpoint/gen.pth.tar"
```

#### 4.4.2 Training Function

```python
# Cell 42 - Training Loop
def train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler):
    loop = tqdm(loader, leave=True)

    for idx, (x, y) in enumerate(loop):
        x = x.to(config.DEVICE)  # Edge images
        y = y.to(config.DEVICE)  # Real face images

        # ====== Train Discriminator ======
        with torch.cuda.amp.autocast():
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏õ‡∏•‡∏≠‡∏°‡∏à‡∏≤‡∏Å Generator
            y_fake = gen(x)

            # D ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ real pair
            D_real = disc(x, y)
            D_real_loss = bce(D_real, torch.ones_like(D_real))

            # D ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ fake pair
            D_fake = disc(x, y_fake.detach())
            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))

            # Total Discriminator loss
            D_loss = (D_real_loss + D_fake_loss) / 2

        disc.zero_grad()
        d_scaler.scale(D_loss).backward()
        d_scaler.step(opt_disc)
        d_scaler.update()

        # ====== Train Generator ======
        with torch.cuda.amp.autocast():
            # D ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ fake pair (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ D ‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤ fake ‡πÄ‡∏õ‡πá‡∏ô real)
            D_fake = disc(x, y_fake)
            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))

            # L1 loss ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏û generated ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á
            L1 = l1_loss(y_fake, y) * config.L1_LAMBDA

            # Total Generator loss
            G_loss = G_fake_loss + L1

        opt_gen.zero_grad()
        g_scaler.scale(G_loss).backward()
        g_scaler.step(opt_gen)
        g_scaler.update()

        # Update progress bar
        if idx % 10 == 0:
            loop.set_postfix(
                D_real=torch.sigmoid(D_real).mean().item(),
                D_fake=torch.sigmoid(D_fake).mean().item(),
            )
```

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**

1. **Train Discriminator**:
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏õ‡∏•‡∏≠‡∏° `y_fake` ‡∏à‡∏≤‡∏Å Generator
   - ‡πÉ‡∏´‡πâ D ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ real pair (x, y) ‚Üí label = 1
   - ‡πÉ‡∏´‡πâ D ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ fake pair (x, y_fake) ‚Üí label = 0
   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì loss ‡πÅ‡∏•‡∏∞ update D

2. **Train Generator**:
   - ‡πÉ‡∏´‡πâ D ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ fake pair (x, y_fake) ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ output ‡πÄ‡∏õ‡πá‡∏ô 1
   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì L1 loss ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á y_fake ‡∏Å‡∏±‡∏ö y
   - ‡∏£‡∏ß‡∏° loss ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á (adversarial + L1√ó100)
   - Update G

3. **Mixed Precision Training**:
   - ‡πÉ‡∏ä‡πâ `torch.cuda.amp.autocast()` ‡πÅ‡∏•‡∏∞ `GradScaler` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß

#### 4.4.3 Optimizer ‡πÅ‡∏•‡∏∞ Loss Functions

```python
# Cell 41 - Setup
opt_disc = optim.Adam(disc.parameters(), lr=config.LEARNING_RATE, betas=(0.5, 0.999))
opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.5, 0.999))

BCE = nn.BCEWithLogitsLoss()
L1_LOSS = nn.L1Loss()

g_scaler = torch.cuda.amp.GradScaler()
d_scaler = torch.cuda.amp.GradScaler()
```

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- **Optimizer**: Adam with Œ≤‚ÇÅ=0.5, Œ≤‚ÇÇ=0.999 (standard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GANs)
- **Learning Rate**: 2e-4
- **Loss Functions**:
  - BCEWithLogitsLoss: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Discriminator (‡∏£‡∏ß‡∏° Sigmoid ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ)
  - L1Loss: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pixel-wise reconstruction

### 4.5 ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Inference

```python
# Inference function
def generate_face(model, edge_image_path, output_path, device="cuda"):
    # Setup transforms
    transform = A.Compose([
        A.Resize(width=256, height=256),
        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),
        ToTensorV2(),
    ])

    # Load ‡πÅ‡∏•‡∏∞ preprocess ‡∏†‡∏≤‡∏û edge
    edge_img = Image.open(edge_image_path).convert("RGB")
    edge_array = np.array(edge_img)
    transformed = transform(image=edge_array)
    input_tensor = transformed["image"].unsqueeze(0).to(device)

    # Generate face
    model.eval()
    with torch.no_grad():
        generated = model(input_tensor)

    # De-normalize ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    generated = generated * 0.5 + 0.5  # [-1, 1] ‚Üí [0, 1]
    save_image(generated, output_path)
```

**‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û edge ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ preprocessing ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô training
- ‡πÉ‡∏ä‡πâ `model.eval()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î dropout ‡πÅ‡∏•‡∏∞ batch normalization
- ‡πÉ‡∏ä‡πâ `torch.no_grad()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
- De-normalize ‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô [0, 1] ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û

---

## 5. ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ Train ‡πÅ‡∏•‡∏∞ Dataset

### 5.1 Dataset

#### 5.1.1 ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ **Cityscape Dataset** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô benchmark dataset ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô Semantic Segmentation ‡πÅ‡∏•‡∏∞ Image Synthesis ‡∏Ç‡∏≠‡∏á Urban Street scenes:

**Cityscape Dataset (Urban Street)**
- **URL**: https://www.kaggle.com/datasets/electraawais/cityscape-dataset
- **‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**: ‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢‡∏ñ‡∏ô‡∏ô‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏à‡∏≤‡∏Å 50 ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÉ‡∏ô‡∏¢‡∏∏‡πÇ‡∏£‡∏õ
- **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**: 1024√ó2048 pixels (High Resolution)
- **Semantic Classes**: 19 classes (road, sidewalk, building, wall, fence, pole, traffic light, traffic sign, vegetation, terrain, sky, person, rider, car, truck, bus, train, motorcycle, bicycle)

**‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**:
- **Training set**: ~3,000 images ‡∏û‡∏£‡πâ‡∏≠‡∏° fine annotations
- **Validation set**: ~500 images ‡∏û‡∏£‡πâ‡∏≠‡∏° fine annotations
- **Test set**: ~1,500 images (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö evaluation ‡∏ö‡∏ô benchmark leaderboard)

**‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå**:
- **Input** (Segmentation Labels): `*_gtFine_color.png` - ‡∏†‡∏≤‡∏û Semantic Segmentation ‡πÅ‡∏ö‡∏ö color-coded
- **Target** (Real Photos): `*_leftImg8bit.png` - ‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡πÄ‡∏°‡∏∑‡∏≠‡∏á

#### 5.1.2 Data Loading Pipeline

‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Cityscape Dataset ‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ‡πÅ‡∏•‡πâ‡∏ß:

1. **‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå**:
   ```
   /workspace/train/data/
   ‚îú‚îÄ‚îÄ input/
   ‚îÇ   ‚îú‚îÄ‚îÄ aachen_000000_gtFine_color.png
   ‚îÇ   ‚îú‚îÄ‚îÄ aachen_000001_gtFine_color.png
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îî‚îÄ‚îÄ label/
       ‚îú‚îÄ‚îÄ aachen_000000_leftImg8bit.png
       ‚îú‚îÄ‚îÄ aachen_000001_leftImg8bit.png
       ‚îî‚îÄ‚îÄ ...
   ```

2. **Data Loading Process**:
   ```
   Input: Segmentation Map (*_gtFine_color.png) ‚Üí Load RGB ‚Üí Convert to Tensor ‚Üí Normalize
   Target: Real Photo (*_leftImg8bit.png) ‚Üí Load RGB ‚Üí Convert to Tensor ‚Üí Normalize
   ```

3. **Image Preprocessing**:
   - ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢ `PIL.Image.open()`
   - ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô RGB mode
   - ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô NumPy array
   - Permute dimensions: (H, W, C) ‚Üí (C, H, W)
   - Normalize: ‡πÅ‡∏ö‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢ 255.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏ä‡πà‡∏ß‡∏á [0, 1]

#### Diagram: Data Loading Pipeline

```mermaid
graph TB
    Start[Cityscape Dataset<br/>3000 train images] --> InputPath[Load Input Path<br/>gtFine_color.png]
    Start --> LabelPath[Load Label Path<br/>leftImg8bit.png]

    InputPath --> LoadInput[PIL.Image.open<br/>Convert to RGB]
    LabelPath --> LoadLabel[PIL.Image.open<br/>Convert to RGB]

    LoadInput --> NPInput[Convert to NumPy<br/>np.array]
    LoadLabel --> NPLabel[Convert to NumPy<br/>np.array]

    NPInput --> TensorInput[Convert to Tensor<br/>permute 2,0,1<br/>1024x2048x3 to 3x1024x2048]
    NPLabel --> TensorLabel[Convert to Tensor<br/>permute 2,0,1<br/>1024x2048x3 to 3x1024x2048]

    TensorInput --> NormInput[Normalize<br/>divide 255.0<br/>Range 0 to 1]
    TensorLabel --> NormLabel[Normalize<br/>divide 255.0<br/>Range 0 to 1]

    NormInput --> Pair[Create Pair<br/>Input, Target]
    NormLabel --> Pair

    Pair --> Dataloader[PyTorch DataLoader<br/>Batch Size 8]

    style Start fill:#e1f5ff
    style LoadInput fill:#ffeaa7
    style LoadLabel fill:#e1ffe1
    style Pair fill:#ffe1f5
    style Dataloader fill:#dfe6e9
```

#### 5.1.3 Data Augmentation

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ augmentation ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå:

- **‡πÑ‡∏°‡πà‡∏°‡∏µ Geometric Augmentation**: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ horizontal flip ‡∏´‡∏£‡∏∑‡∏≠ rotation ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡∏î‡∏π‡∏ú‡∏¥‡∏î‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏ñ‡∏ß‡∏¥‡πà‡∏á‡∏ú‡∏¥‡∏î‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏ô‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô)
- **‡πÑ‡∏°‡πà‡∏°‡∏µ Color Augmentation**: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ color jitter ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏™‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ï‡πà‡∏≤‡∏á‡πÜ
- **Normalization ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏à‡∏≤‡∏Å [0, 255] ‚Üí [0, 1] ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ 255.0

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏**: Cityscape Dataset ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (~3,000 images) ‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ aggressive augmentation

### 5.2 Training Configuration

#### 5.2.1 Hyperparameters

| Parameter | Value | ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|-----------|-------|--------|
| Batch Size | 8 | ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å 4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠ training ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏∂‡πâ‡∏ô |
| Learning Rate | 2e-4 | Standard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GANs (Adam optimizer) |
| Optimizer | Adam | Œ≤‚ÇÅ=0.5, Œ≤‚ÇÇ=0.999 |
| L1 Lambda | 100 | Weight ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1 loss ‡πÉ‡∏ô Generator |
| Epochs | 60
| Train ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ loss ‡∏à‡∏∞ converge |
| Image Size | 1024√ó2048 | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á Cityscape Dataset |
| Num Workers | 2 | ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DataLoader parallel loading |

#### 5.2.2 Training Environment

- **Platform**: Google Colab Pro
- **GPU**: Tesla T4 / V100 (16 GB VRAM)
- **Framework**: PyTorch 1.13+
- **Mixed Precision**: Enabled (FP16)
- **‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Train**: ~12-15 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

#### 5.2.3 Training Strategy

1. **Warm-up**: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ learning rate ‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏°‡∏µ warm-up
2. **Batch Update**: Update D ‡πÅ‡∏•‡∏∞ G ‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å batch
3. **Checkpoint**: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å model ‡∏ó‡∏∏‡∏Å 5 epochs
4. **Early Stopping**: Monitor validation loss ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ automatic early stopping
5. **Resume Training**: ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ load checkpoint ‡πÅ‡∏•‡∏∞ train ‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ

---

## 6. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• (Evaluation)

### 6.1 Loss Curves

#### 6.1.1 Generator Loss

```
Generator Total Loss = Adversarial Loss + L1 Loss √ó 100

Epoch 1:  G_loss ‚âà 50-60
Epoch 10: G_loss ‚âà 30-40
Epoch 50: G_loss ‚âà 15-20
Epoch 90: G_loss ‚âà 10-15
```

**‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:**
- L1 loss ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ generated images ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á ground truth ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
- Adversarial loss ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô GAN training

#### 6.1.2 Discriminator Loss

```
Discriminator Loss = (Real Loss + Fake Loss) / 2

Epoch 1:  D_loss ‚âà 0.5-0.7
Epoch 10: D_loss ‚âà 0.3-0.5
Epoch 90: D_loss ‚âà 0.2-0.4
```

**‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:**
- D loss ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 0.3-0.5 ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ D ‡πÅ‡∏•‡∏∞ G ‡∏°‡∏µ balance ‡∏ó‡∏µ‡πà‡∏î‡∏µ
- ‡∏ñ‡πâ‡∏≤ D loss ‚Üí 0: D ‡πÅ‡∏£‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- ‡∏ñ‡πâ‡∏≤ D loss ‚Üí 1: G ‡πÅ‡∏£‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

#### 6.1.3 Discriminator Predictions

```
D(real) ‚âà 0.8-0.9  (‡∏Ñ‡∏ß‡∏£‡πÉ‡∏Å‡∏•‡πâ 1)
D(fake) ‚âà 0.1-0.3  (‡∏Ñ‡∏ß‡∏£‡πÉ‡∏Å‡∏•‡πâ 0)
```

### 6.2 Qualitative Evaluation

#### 6.2.1 Visual Quality Assessment

‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏Å‡∏ì‡∏ë‡πå:

1. **Realism (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á)**:
   - ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
   - ‡∏™‡∏µ‡∏ú‡∏¥‡∏ß ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ú‡∏¥‡∏ß ‡∏î‡∏π‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
   - ‡πÑ‡∏°‡πà‡∏°‡∏µ artifacts ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î

2. **Structure Consistency (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á)**:
   - ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤ ‡∏à‡∏°‡∏π‡∏Å ‡∏õ‡∏≤‡∏Å ‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏™‡πâ‡∏ô‡∏Ç‡∏≠‡∏ö
   - ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

3. **Detail Preservation (‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î)**:
   - ‡∏Ñ‡∏¥‡πâ‡∏ß ‡∏Ç‡∏ô‡∏ï‡∏≤ ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤
   - ‡πÄ‡∏á‡∏≤ ‡πÅ‡∏™‡∏á ‡∏°‡∏µ depth ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

#### 6.2.2 ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

**‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡πâ‡∏ô‡∏Ç‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Training Set:**
- ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
- L1 loss ‡∏ó‡∏≥‡πÉ‡∏´‡πâ generated image ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á ground truth

**‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡πâ‡∏ô‡∏Ç‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ß‡∏≤‡∏î‡πÄ‡∏≠‡∏á:**
- ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ variation ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
- ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏°‡∏µ artifacts ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
- ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏Ç‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ß‡∏≤‡∏î

### 6.3 Quantitative Metrics (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ implement metrics ‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ:

1. **PSNR (Peak Signal-to-Noise Ratio)**:
   - ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏û generated ‡∏Å‡∏±‡∏ö ground truth
   - ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á = ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ

2. **SSIM (Structural Similarity Index)**:
   - ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û
   - ‡∏Ñ‡πà‡∏≤ 0-1, ‡πÉ‡∏Å‡∏•‡πâ 1 = ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏≤‡∏Å

3. **FID (Fr√©chet Inception Distance)**:
   - ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á distribution ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û generated
   - ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥ = ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ

4. **IS (Inception Score)**:
   - ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û generated
   - ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á = ‡∏î‡∏µ

### 6.4 Limitations

1. **Mode Collapse**: ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
2. **Artifacts**: ‡∏°‡∏µ artifacts ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏ö‡∏†‡∏≤‡∏û
3. **Diversity**: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û output ‡∏≠‡∏≤‡∏à‡∏à‡∏≥‡∏Å‡∏±‡∏î
4. **Edge Sensitivity**: ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏Ç‡∏≠‡∏ö input

---

## 7. ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

### 7.1 ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á

1. **Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks**
   - ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros
   - ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡∏µ‡∏û‡∏¥‡∏°‡∏û‡πå: CVPR 2017
   - Link: https://arxiv.org/abs/1611.07004
   - **‡∏™‡∏£‡∏∏‡∏õ**: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Pix2Pix framework ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö image-to-image translation ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Conditional GANs ‡∏Å‡∏±‡∏ö PatchGAN discriminator ‡πÅ‡∏•‡∏∞ U-Net generator

2. **U-Net: Convolutional Networks for Biomedical Image Segmentation**
   - ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô: Olaf Ronneberger, Philipp Fischer, Thomas Brox
   - ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡∏µ‡∏û‡∏¥‡∏°‡∏û‡πå: MICCAI 2015
   - Link: https://arxiv.org/abs/1505.04597
   - **‡∏™‡∏£‡∏∏‡∏õ**: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° U-Net ‡∏ó‡∏µ‡πà‡∏°‡∏µ skip connections ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô image segmentation ‡πÅ‡∏•‡∏∞ generation

3. **Generative Adversarial Networks**
   - ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô: Ian Goodfellow et al.
   - ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡∏µ‡∏û‡∏¥‡∏°‡∏û‡πå: NeurIPS 2014
   - Link: https://arxiv.org/abs/1406.2661
   - **‡∏™‡∏£‡∏∏‡∏õ**: ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏Ç‡∏≠‡∏á GANs ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ adversarial training framework

### 7.2 ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

1. **GauGAN (Nvidia)**: Semantic Image Synthesis
   - ‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤: NVIDIA Research
   - ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏ï‡∏±‡∏ß: 2019 (GauGAN), 2021 (GauGAN2)
   - Link: https://www.nvidia.com/en-us/research/ai-playground/
   - **‡∏™‡∏£‡∏∏‡∏õ**: ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏à‡∏≤‡∏Å Nvidia ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Spatially-Adaptive Normalization (SPADE) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å semantic maps ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ landscape ‡πÅ‡∏•‡∏∞ scene generation
   - **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ**:
     - GauGAN ‡πÉ‡∏ä‡πâ SPADE layers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö normalization ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á semantic information
     - ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ Pix2Pix architecture ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞ U-Net generator
     - GauGAN ‡∏°‡∏µ style control ‡πÅ‡∏•‡∏∞ multimodal generation
     - ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡πÄ‡∏ô‡πâ‡∏ô urban street scenes ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≤‡∏Å Cityscape Dataset

2. **CycleGAN**: Unpaired Image-to-Image Translation
   - ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Pix2Pix ‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ paired data
   - ‡πÉ‡∏ä‡πâ cycle consistency loss

3. **StyleGAN**: High-quality face generation
   - Focus ‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
   - ‡∏°‡∏µ style control ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤

4. **ProGAN**: Progressive Growing of GANs
   - Train ‡πÅ‡∏ö‡∏ö progressive ‡∏à‡∏≤‡∏Å resolution ‡∏ï‡πà‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡∏á
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡πÑ‡∏î‡πâ

5. **pix2pixHD**: High-Resolution Image Synthesis
   - ‡∏Ç‡∏¢‡∏≤‡∏¢ Pix2Pix ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö resolution ‡∏™‡∏π‡∏á (2048√ó1024)
   - ‡πÉ‡∏ä‡πâ multi-scale discriminators

### 7.3 Datasets

1.  https://www.kaggle.com/datasets/electraawais/cityscape-dataset/code/data

### 7.4 Tutorials ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

1. **TensorFlow Pix2Pix Tutorial**
   - https://www.tensorflow.org/tutorials/generative/pix2pix
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ implementation ‡πÅ‡∏ö‡∏ö step-by-step

2. **PyTorch Image-to-Image Translation**
   - https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
   - Official implementation ‡∏Ç‡∏≠‡∏á Pix2Pix ‡πÅ‡∏•‡∏∞ CycleGAN

---

## 8. ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠

### 8.1 ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏á‡∏≤‡∏ô

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ Seg2Scene ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£:

1. ‚úÖ **‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Pix2Pix GAN** ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á Semantic Segmentation Maps ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
2. ‚úÖ **‡πÉ‡∏ä‡πâ Cityscape Dataset** ~3,000 training images ‡∏û‡∏£‡πâ‡∏≠‡∏° high-resolution segmentation labels
3. ‚úÖ **Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•** ‡∏ö‡∏ô Google Colab ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 60 epochs ‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏û‡∏Ç‡∏ô‡∏≤‡∏î 1024√ó2048
4. ‚úÖ **‡∏ó‡∏≥‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö** ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏° architecture diagrams

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ:**
- ‡πÉ‡∏ä‡πâ state-of-the-art architecture (Pix2Pix GAN)
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏¥‡∏ß‡∏ó‡∏±‡∏®‡∏ô‡πå‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö high-resolution images (1024√ó2048)
- ‡∏°‡∏µ applications ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô Urban Planning, Autonomous Driving, Virtual Environments

**‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î:**
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ GPU ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ train (‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡πÉ‡∏´‡∏ç‡πà)
- Model size ‡πÉ‡∏´‡∏ç‡πà (~600 MB)
- ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏°‡∏µ artifacts ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏•‡πá‡∏Å‡πÜ (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå, ‡∏õ‡πâ‡∏≤‡∏¢‡∏à‡∏£‡∏≤‡∏à‡∏£)
- Training time ‡∏ô‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡πÉ‡∏´‡∏ç‡πà

### 8.2 ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠

1. **‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏û:**
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° resolution ‡πÄ‡∏õ‡πá‡∏ô 512√ó512 ‡∏´‡∏£‡∏∑‡∏≠ 1024√ó1024
   - ‡πÉ‡∏ä‡πâ Progressive Growing approach
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° Perceptual Loss ‡πÅ‡∏ó‡∏ô... (8 KB left)

